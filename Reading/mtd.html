<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>无标题文档</title>
</head>

<body>
<h1>Mining Text Data</h1>
<p>this book is written by a group of researcher, where the main authors are Charu C. Aggarwal and ChengXiang Zhai</p>
<h3>Chapter One: An Introduction to  Data Mining</h3>
<h4>Introduction</h4>Data mining can learn interesting  patterns from the data in a dynamic and scalable way. Information retrieval has  traditionally focused more on facilitating information access rather than  analyzing information to discover patterns, which is the primary goal of text  mining.<br />
The most important characteristic of  text data is sparse and high dimensional due to string input. Furthermore, in  most application, it would be desirable to represent text information  semantically, however, the natural language processing are still not robust  enough to work. Usually, text data will be treated as a bag-of-words or a  string of words.<br />
Recently, there has been rapid growth of  text data in the context of different web-based applications such as social  media<br />
<h4>Algorithm  for text mining </h4>
1, information extraction from text data<br />
2, text summarization<br />
3, unsupervised learning methods from  text data: clustering and topic modeling<br />
4, LSI and dimensionality reduction for  text mining<br />
5, supervised learning methods from test  data: classification and transfer learning<br />
6, transfer learning with text data: for  cross-lingual mining in some web source<br />
7, probabilistic techniques for test  mining<br />
8, mining text streams: for Reuters and news<br />
9, cross-lingual mining of text data<br />
10, text mining in multimedia networks<br />
11, text mining in social media<br />
12, opinion mining from text data<br />
13, text mining from biomedical data<br />
<h4>Future  Direction</h4>
1, Scalable and robust methods for  natural language understanding: It is important to develop effective and robust  information extraction and other natural language processing methods that can  scale to multiple domains<br />
2, Domain adaptation and transfer  learning<br />
3, Contextual analysis of text data: Text  data is generally associated with a lot of context information such as authors,  sources, and time, or more complicated information networks associated with text  data.<br />
4, Parallel text mining: In particular,  how to parallelize all kinds of text mining algorithms, including both  unsupervised and supervised learning methods is a major future challenge.

<h3><br />
Chapter two: Information  Extraction from Text</h3>
Information extraction in this chapter  mainly focuses on named entities recognition and relation extraction<br />
<h4>Introduction</h4>
E.g. in 1998, Larry Page and Sergey Brin  founded Google Inc. where there are relation between named enetities, such as  FounderOf ( Larry Page, Google Inc.), FounderOf (Sergey Brin, Google Inc.), FoundedIn(Google  Inc., 1998 ).<br />
Some examples of application:<br />
1, biomedical researchers often need to  sift through a large amount of scientific publications to look for discoveries  related to particular genes, proteins or other biomedical entities.<br />
2, Financial professionals often need to  seek specific pieces of information from news articles to help their day-to-day  decision making.<br />
3, Intelligence analysts review large  amounts of text to search for information such as people involved in terrorism  events, the weapons used and the targets of the attacks.<br />
4, With the fast growth of the Web,  search engines have become an integral part of people’s daily lives, and users’  search behaviors are much better understood now.<br />
The main conference MUC (short for  Message Understanding Conference) focuses on named entities recognition and  relation extraction. Early MUCs defined information extraction as filling a  predefined template that contains a set of predefined slots. Then, rule-based  systems participate in MUC. And with the decomposition of information  extraction systems into components such as named entity recognition, many  information extraction subtasks can be transformed into classification  problems, which can be solved by standard supervised learning algorithms such  as support vector machines and maximum entropy models.<br />
Another new direction is open information  extraction, where the system is expected to extract all useful entity relations  from a large, diverse corpus such as the Web. Recent advances in this direction  include system like TEXTRUNNER, WOE and REVERB<br />
<h4>Named  Entity Recognition</h4>
All the named entities have two characteristics  those are open set and context dependent.<br />
<strong>Rule-based  Approach</strong><br />
A rule consists of a pattern and an  action. It is possible for a sequence of tokens to match multiple rules. To handle  such conflicts, a set of policies has to be defined to control how rules should  be fired. Manually creating the rules for named entity recognition requires  human expertise and is labor intensive.<br />
Top-down approach causes low precision  and bottom-up approach causes low coverage.<br />
<strong>Statistical  Learning Approach</strong><br />
This approach regards recognition as a sequence  labeling task. Some available method can be used, HMM, MEMM and linear CRF.<br />
In HMM, one way to model the joint probability  is to assume a Markov process where the generation of a label or an observation  is dependent only on one or a few previous labels and/or observations.<br />
MEMM is the maximum entropy model  coupled with a Markovian assumption, which is a shift from generative models to  discriminative models.<br />
CRF is popular discriminative model for  sequence labeling, and the difference from MEMM is that in CRFs the label of  the current observation can depend not only on previous labels but also on  future labels. However, in linear-chain CRF, long-range feature cannot be  defined in CRF, so semi-Markov CRF will perform better, which is proved in one  work.<br />
<h4>Relation  Extraction</h4>
A set of major relation types and their  subtypes are defined by ACE (short for Annotation Context Extraction). ACE  makes a distinction between relation extraction and relation mention  extraction. The former refers to identifying the semantic relation between a  pair of entities based on all the evidence we can gather from the corpus,  whereas the latter refers to identifying individual mentions of entity  relations. Because corpus-level relation extraction to a large extent still  relies on accurate mention-level relation extraction,<br />
<strong>Feature-based  classification</strong><br />
A two-stage classification can be performed  where at the first stage whether two entities are related is determined and at  the second stage the relation type for each related entity pair is determined.<br />
Some of the most commonly used features as  follows:<br />
1, entity features<br />
2, lexical contextual features<br />
3, syntactic contextual features: Syntactic  relations between the two arguments or between an argument and another word can  often be useful.<br />
4, background knowledge: ontology, Wikipedia  or DBpedia<br />
A framework to organize the features  used for relation extraction is proposed. A relation instance is represented as  a labeled, directed graph G = (V, E, A, B), where V is the set of nodes in the  graph, E is the set of directed edges in the graph, and A (feature value corresponding  to feature set) and B (distinguish the node type) are functions that assign  labels to the nodes. It is found that a combination of features at different  levels of complexity and from different sentence representations, coupled with  task-oriented feature pruning, gave the best performance.<br />
<strong>Kernel  methods</strong><br />
In machine learning, a kernel or kernel  function defines the inner product of two observed instances represented in  some underlying vector space. It can also be seen as a similarity measure for  the observations. The major advantage of using kernels is that observed  instances do not need to be explicitly mapped to the underlying vector space in  order for their inner products defined by the kernel to be computed.<br />
1, sequence-based kernel: the shortest  path in dependency tree. However, the paths with different length will be ended  with zero similarity.<br />
2, tree-based kernel: The main  motivation is that if two parse trees share many common subtree structures then  the two relation instances are similar to each other.<br />
3, composite kernel: combine 1 and 2<br />
<strong>Weakly  supervised (semi-supervised)</strong> <strong>learning method </strong><br />
Weakly supervised learning methods are  the work with much less training data.<br />
1, bootstrapping: Given a large corpus,  we then look for co-occurrences of these entity pairs within close proximity.  The assumption is that if two entities related through the target relation  co-occur closely, the context in which they co-occur is likely to be a pattern  for the target relation. An important step in bootstrapping methods is to  evaluate the quality of extraction patterns (heuristic methods) so as not to  include many noisy patterns during the extraction process.<br />
2, distant supervision: usage of  knowledge bases<br />
<h4>Unsupervised  Information Extraction</h4>
The key idea is to cluster entities or  entity pairs based on their lexical-syntactic contextual features.<br />
<strong>Relation  discovery and template induction</strong><br />
Work example: They started by collecting  a large number of news articles from different news sources on the Web. They  then used simple clustering based on lexical similarity to find articles  talking about the same event. Next they performed syntactic parsing and  extracted named entities from these articles. Each named entity could then be represented  by a set of syntactic patterns as its features. Finally, they clustered pairs  of entities co-occurring in the same article using their feature  representations. The end results were tables in which rows corresponded to  different articles and columns corresponded to different roles in a relation.<br />
With relation discovery, the most  straightforward solution is to identify candidates of role fillers first and  then cluster these candidates into clusters. However, the candidates can belong  to many clusters with the same features. Later, prior label assignment with  probability is defined.<br />
The aforementioned studies cannot label  the discovered slots. A method to this problem is that performs two steps of  clustering where the first clustering step groups lexical patterns that are  likely to describe the same type of events and the second clustering step  groups candidate role fillers into slots for each type of events. A slot can be  labeled using the syntactic patterns of the corresponding slot fillers.<br />
<strong>Open  Information Extraction</strong><br />
In order to alleviate the situation those  relation discovery and template inductions usually work on a corpus from a  single domain, it need open information extraction methods.<br />
Open information extraction does not  assume any specific target relation type. Recent work on open information  extraction introduced more heuristics to improve the quality of the extracted  relations, such as 1 A multi-word relation phrase must begin with a verb, end  with a preposition, and be a contiguous sequence of words in the sentence; 2 A  binary relation phrase ought to appear with at least a minimal number of  distinct argument pairs in a large corpus.<br />
<h4>Evaluation</h4>
For named entity recognition, strictly speaking  a correctly identified named entity must satisfy two criteria, namely, correct  entity boundary and correct entity type.<br />
For relation extraction, as we have mentioned,  there are two levels of extraction, corpus-level and mention-level. While  evaluation at mention level requires annotated relation mention instances,  evaluation at corpus level requires only truly related entity pairs, which may  be easier to obtain or annotate than relation mentions.
</p>

<p>&nbsp;</p>
</body>
</html>
